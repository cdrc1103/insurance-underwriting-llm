{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Analysis for Insurance Underwriting Dataset\n",
    "\n",
    "This notebook documents the token analysis performed for User Story 1.5, including:\n",
    "- Token count statistics for all splits\n",
    "- Recommended max_length determination\n",
    "- Truncation analysis\n",
    "- Edge case identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datasets import load_from_disk\n\nfrom src.data.tokenization import (\n    format_messages_for_tokenization,\n    load_tokenizer,\n)\n\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the token analysis results\nresults_path = Path(\"../results/token_analysis.json\")\n\nwith open(results_path) as f:\n    results = json.load(f)\n\nprint(\"Token Analysis Results\")\nprint(\"=\" * 80)\nprint(f\"Model: {results['model_name']}\")\nprint(f\"Tokenizer vocab size: {results['tokenizer_vocab_size']:,}\")\nprint(f\"Recommendation percentile: {results['recommendation_percentile']}th\")\nprint(f\"Recommended max_length: {results['recommended_max_length']:,} tokens\")\nprint()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Statistics Summary\n",
    "\n",
    "Below are the comprehensive token statistics for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN Split (222 examples)\n",
      "------------------------------------------------------------\n",
      "  Mean: 5425.1 tokens\n",
      "  Median: 2441.5 tokens\n",
      "  Min: 472 tokens\n",
      "  Max: 64,258 tokens\n",
      "  Std Dev: 7728.4 tokens\n",
      "\n",
      "  Percentiles:\n",
      "    25th: 1,698.2 tokens\n",
      "    50th: 2,441.5 tokens\n",
      "    75th: 4,786.8 tokens\n",
      "    90th: 19,108.3 tokens\n",
      "    95th: 21,486.0 tokens\n",
      "    99th: 27,636.1 tokens\n",
      "\n",
      "VALIDATION Split (37 examples)\n",
      "------------------------------------------------------------\n",
      "  Mean: 4086.6 tokens\n",
      "  Median: 2119.0 tokens\n",
      "  Min: 804 tokens\n",
      "  Max: 20,415 tokens\n",
      "  Std Dev: 4728.0 tokens\n",
      "\n",
      "  Percentiles:\n",
      "    25th: 1,718.0 tokens\n",
      "    50th: 2,119.0 tokens\n",
      "    75th: 4,020.0 tokens\n",
      "    90th: 8,333.0 tokens\n",
      "    95th: 16,624.6 tokens\n",
      "    99th: 20,227.8 tokens\n",
      "\n",
      "TEST Split (37 examples)\n",
      "------------------------------------------------------------\n",
      "  Mean: 5430.5 tokens\n",
      "  Median: 2564.0 tokens\n",
      "  Min: 1205 tokens\n",
      "  Max: 29,997 tokens\n",
      "  Std Dev: 6757.8 tokens\n",
      "\n",
      "  Percentiles:\n",
      "    25th: 1,600.0 tokens\n",
      "    50th: 2,564.0 tokens\n",
      "    75th: 7,049.0 tokens\n",
      "    90th: 10,707.4 tokens\n",
      "    95th: 22,294.6 tokens\n",
      "    99th: 28,695.2 tokens\n"
     ]
    }
   ],
   "source": [
    "# Display token statistics for all splits\n",
    "for split_name, stats in results[\"token_statistics\"].items():\n",
    "    print(f\"\\n{split_name.upper()} Split ({stats['total_examples']} examples)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Mean: {stats['mean']:.1f} tokens\")\n",
    "    print(f\"  Median: {stats['median']:.1f} tokens\")\n",
    "    print(f\"  Min: {stats['min']} tokens\")\n",
    "    print(f\"  Max: {stats['max']:,} tokens\")\n",
    "    print(f\"  Std Dev: {stats['std']:.1f} tokens\")\n",
    "    print(\"\\n  Percentiles:\")\n",
    "    for pct, value in stats[\"percentiles\"].items():\n",
    "        print(f\"    {pct}: {value:,.1f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Token Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer and splits for visualization\ntokenizer = load_tokenizer(\"Qwen/Qwen3-0.6B\")\n\nsplits = {}\nfor split_name in [\"train\", \"validation\", \"test\"]:\n    split_path = Path(\"../data/splits\") / split_name\n    splits[split_name] = load_from_disk(str(split_path))\n\n# Compute token counts for each split\ntoken_counts = {}\nfor split_name, split in splits.items():\n    counts = []\n    for example in split:\n        text = format_messages_for_tokenization(example[\"messages\"], tokenizer)\n        tokens = tokenizer(text, return_tensors=None)\n        counts.append(len(tokens[\"input_ids\"]))\n    token_counts[split_name] = counts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle(\"Token Distribution Analysis\", fontsize=16, fontweight=\"bold\")\n\n# 1. Combined histogram\nax = axes[0, 0]\nfor split_name, counts in token_counts.items():\n    ax.hist(counts, bins=30, alpha=0.6, label=split_name.capitalize())\nax.axvline(\n    x=results[\"recommended_max_length\"],\n    color=\"red\",\n    linestyle=\"--\",\n    linewidth=2,\n    label=f\"Recommended max_length ({results['recommended_max_length']:,})\",\n)\nax.set_xlabel(\"Token Count\")\nax.set_ylabel(\"Frequency\")\nax.set_title(\"Token Count Distribution (All Splits)\")\nax.legend()\nax.grid(alpha=0.3)\n\n# 2. Box plot\nax = axes[0, 1]\nax.boxplot(\n    [token_counts[\"train\"], token_counts[\"validation\"], token_counts[\"test\"]],\n    labels=[\"Train\", \"Validation\", \"Test\"],\n)\nax.axhline(\n    y=results[\"recommended_max_length\"],\n    color=\"red\",\n    linestyle=\"--\",\n    linewidth=2,\n    label=f\"Max length: {results['recommended_max_length']:,}\",\n)\nax.set_ylabel(\"Token Count\")\nax.set_title(\"Token Count Distribution by Split\")\nax.legend()\nax.grid(alpha=0.3)\n\n# 3. Cumulative distribution\nax = axes[1, 0]\nfor split_name, counts in token_counts.items():\n    sorted_counts = np.sort(counts)\n    cumulative = np.arange(1, len(sorted_counts) + 1) / len(sorted_counts) * 100\n    ax.plot(sorted_counts, cumulative, label=split_name.capitalize(), linewidth=2)\nax.axvline(\n    x=results[\"recommended_max_length\"],\n    color=\"red\",\n    linestyle=\"--\",\n    linewidth=2,\n    label=f\"Max length: {results['recommended_max_length']:,}\",\n)\nax.axhline(y=95, color=\"orange\", linestyle=\":\", linewidth=1, label=\"95th percentile\")\nax.set_xlabel(\"Token Count\")\nax.set_ylabel(\"Cumulative Percentage\")\nax.set_title(\"Cumulative Distribution\")\nax.legend()\nax.grid(alpha=0.3)\n\n# 4. Truncation impact bar chart\nax = axes[1, 1]\ntruncation_data = results[\"truncation_analysis\"]\nsplits_list = list(truncation_data.keys())\ntruncated_counts = [truncation_data[s][\"truncated_count\"] for s in splits_list]\ntotal_counts = [truncation_data[s][\"total_examples\"] for s in splits_list]\nx = np.arange(len(splits_list))\nwidth = 0.35\nax.bar(x - width / 2, total_counts, width, label=\"Total Examples\", alpha=0.8)\nax.bar(x + width / 2, truncated_counts, width, label=\"Truncated\", alpha=0.8, color=\"orange\")\nax.set_xlabel(\"Split\")\nax.set_ylabel(\"Count\")\nax.set_title(f\"Truncation Impact (max_length={results['recommended_max_length']:,})\")\nax.set_xticks(x)\nax.set_xticklabels([s.capitalize() for s in splits_list])\nax.legend()\nax.grid(alpha=0.3, axis=\"y\")\n\n# Add percentage labels on truncated bars\nfor i, (trunc, total) in enumerate(zip(truncated_counts, total_counts)):\n    if trunc > 0:\n        pct = trunc / total * 100\n        ax.text(i + width / 2, trunc, f\"{pct:.1f}%\", ha=\"center\", va=\"bottom\")\n\nplt.tight_layout()\nplt.savefig(Path(\"../results/token_distribution.png\"), dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nprint(\"\\nVisualization saved to: results/token_distribution.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncation Analysis\n",
    "\n",
    "Analysis of examples that will be truncated at the recommended max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation Analysis\n",
      "================================================================================\n",
      "Recommended max_length: 21,486 tokens\n",
      "\n",
      "TRAIN Split:\n",
      "  Total examples: 222\n",
      "  Truncated: 12 (5.4%)\n",
      "  Max tokens over limit: 42,772\n",
      "  Avg tokens over limit: 7665\n",
      "  Truncated indices: [22, 30, 61, 69, 111, 139, 148, 169, 170, 171, 176, 214]\n",
      "\n",
      "VALIDATION Split:\n",
      "  Total examples: 37\n",
      "  Truncated: 0 (0.0%)\n",
      "\n",
      "TEST Split:\n",
      "  Total examples: 37\n",
      "  Truncated: 2 (5.4%)\n",
      "  Max tokens over limit: 8,511\n",
      "  Avg tokens over limit: 6703\n",
      "  Truncated indices: [18, 36]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Truncation Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Recommended max_length: {results['recommended_max_length']:,} tokens\\n\")\n",
    "\n",
    "for split_name, trunc_stats in results[\"truncation_analysis\"].items():\n",
    "    print(f\"{split_name.upper()} Split:\")\n",
    "    print(f\"  Total examples: {trunc_stats['total_examples']}\")\n",
    "    print(\n",
    "        f\"  Truncated: {trunc_stats['truncated_count']} \"\n",
    "        f\"({trunc_stats['truncated_percentage']:.1f}%)\"\n",
    "    )\n",
    "    if trunc_stats[\"truncated_count\"] > 0:\n",
    "        print(f\"  Max tokens over limit: {trunc_stats['max_tokens_over']:,}\")\n",
    "        print(f\"  Avg tokens over limit: {trunc_stats['avg_tokens_over']:.0f}\")\n",
    "        print(f\"  Truncated indices: {trunc_stats['truncated_indices']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Cases: Very Long Conversations\n",
    "\n",
    "Let's examine the longest conversations that will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Cases: Longest Conversations\n",
      "\n",
      "================================================================================\n",
      "\n",
      "TRAIN Split - Top 3 Longest:\n",
      "  1. Index 69:\n",
      "     - Tokens: 64,258\n",
      "     - Turns: 23\n",
      "     - Task: Product Recommendations\n",
      "     - Will be truncated by 42,772 tokens\n",
      "  2. Index 139:\n",
      "     - Tokens: 39,630\n",
      "     - Turns: 23\n",
      "     - Task: Appetite Check\n",
      "     - Will be truncated by 18,144 tokens\n",
      "  3. Index 169:\n",
      "     - Tokens: 27,660\n",
      "     - Turns: 35\n",
      "     - Task: Product Recommendations\n",
      "     - Will be truncated by 6,174 tokens\n",
      "\n",
      "VALIDATION Split - Top 3 Longest:\n",
      "  1. Index 13:\n",
      "     - Tokens: 20,415\n",
      "     - Turns: 41\n",
      "     - Task: Product Recommendations\n",
      "     - Will NOT be truncated\n",
      "  2. Index 24:\n",
      "     - Tokens: 19,895\n",
      "     - Turns: 25\n",
      "     - Task: Product Recommendations\n",
      "     - Will NOT be truncated\n",
      "  3. Index 26:\n",
      "     - Tokens: 15,807\n",
      "     - Turns: 33\n",
      "     - Task: Product Recommendations\n",
      "     - Will NOT be truncated\n",
      "\n",
      "TEST Split - Top 3 Longest:\n",
      "  1. Index 18:\n",
      "     - Tokens: 29,997\n",
      "     - Turns: 37\n",
      "     - Task: Product Recommendations\n",
      "     - Will be truncated by 8,511 tokens\n",
      "  2. Index 36:\n",
      "     - Tokens: 26,381\n",
      "     - Turns: 15\n",
      "     - Task: Small Business Elibility Check\n",
      "     - Will be truncated by 4,895 tokens\n",
      "  3. Index 25:\n",
      "     - Tokens: 21,273\n",
      "     - Turns: 21\n",
      "     - Task: Product Recommendations\n",
      "     - Will NOT be truncated\n"
     ]
    }
   ],
   "source": [
    "# Identify edge cases (very long conversations)\n",
    "print(\"Edge Cases: Longest Conversations\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for split_name, counts in token_counts.items():\n",
    "    # Get top 3 longest conversations\n",
    "    indices_sorted = np.argsort(counts)[::-1][:3]\n",
    "\n",
    "    print(f\"\\n{split_name.upper()} Split - Top 3 Longest:\")\n",
    "    for rank, idx in enumerate(indices_sorted, 1):\n",
    "        example = splits[split_name][int(idx)]\n",
    "        token_count = counts[int(idx)]\n",
    "        num_turns = len(example[\"messages\"])\n",
    "\n",
    "        print(f\"  {rank}. Index {idx}:\")\n",
    "        print(f\"     - Tokens: {token_count:,}\")\n",
    "        print(f\"     - Turns: {num_turns}\")\n",
    "        print(f\"     - Task: {example.get('task', 'N/A')}\")\n",
    "\n",
    "        if token_count > results[\"recommended_max_length\"]:\n",
    "            tokens_over = token_count - results[\"recommended_max_length\"]\n",
    "            print(f\"     - Will be truncated by {tokens_over:,} tokens\")\n",
    "        else:\n",
    "            print(\"     - Will NOT be truncated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "- **Model**: Qwen/Qwen3-0.6B\n",
    "- **Tokenizer vocab size**: 151,643 tokens\n",
    "- **Recommended max_length**: Based on 95th percentile of training set\n",
    "\n",
    "### Token Distribution\n",
    "\n",
    "The token distribution shows:\n",
    "- Most examples are under 5,000 tokens\n",
    "- There's a long tail with some examples exceeding 20,000 tokens\n",
    "- The median is much lower than the mean, indicating right-skewed distribution\n",
    "\n",
    "### Truncation Impact\n",
    "\n",
    "At the recommended max_length:\n",
    "- Train: ~5% of examples will be truncated\n",
    "- Validation: 0% truncated\n",
    "- Test: ~5% truncated\n",
    "- Overall: ~5% of all examples affected\n",
    "\n",
    "### Recommendations for Training\n",
    "\n",
    "1. **Use recommended max_length**: Balances coverage (95% of examples) with efficiency\n",
    "2. **Monitor truncated examples**: Review truncated examples to ensure critical information isn't lost\n",
    "3. **Consider smart truncation**: Implement smart truncation strategies that preserve company profile and key conversation context\n",
    "4. **Batch size planning**: With max_length ~20K tokens, plan batch size accordingly for T4 16GB GPU (likely batch_size=1-2)\n",
    "\n",
    "### Edge Cases\n",
    "\n",
    "Very long conversations (>25K tokens) should be reviewed to understand:\n",
    "- Are they legitimate complex underwriting scenarios?\n",
    "- Could they be split into multiple examples?\n",
    "- Is there redundant information that could be removed during preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Update\n",
    "\n",
    "Based on this analysis, the following configuration should be used for training:\n",
    "\n",
    "```python\n",
    "# configs/model.py\n",
    "RECOMMENDED_FINE_TUNING_MAX_LENGTH = 21486  # Based on 95th percentile\n",
    "```\n",
    "\n",
    "For memory efficiency on T4 16GB GPU, consider:\n",
    "- Batch size: 1-2\n",
    "- Gradient accumulation steps: 4-8\n",
    "- Effective batch size: 4-16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance-underwriting-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
